# -*- coding: utf-8 -*-
"""disease_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aV_bgjHnHFl1M9UFAKLeeeo8LxkOrIhQ
"""

!unzip -uq "/content/drive/My Drive/image.zip" -d "/content/drive/My Drive"

"""Load dataset and preprocessing"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import cv2

# %matplotlib inline

dirtrain = '/content/drive/My Drive/image/train'

dirtest = '/content/drive/My Drive/image/test'

categories = ["Anthracnose","Downy Mildew","Healthy","Powdery Mildew"]

import matplotlib.pyplot as plt
import os
import cv2
import numpy as np
import pickle
import random

for c in categories:
    path=os.path.join(dirtrain,c)
    for i in os.listdir(path):
        img_array=cv2.imread(os.path.join(path,i))
        break
    break

training_data = []
def create_training_data():
    count=[]
    for c in categories:
        path=os.path.join(dirtrain,c)
        class_num=categories.index(c)
        c=0
        for i in os.listdir(path):
            c=c+1
            try:
                img_array=cv2.imread(os.path.join(path,i))
                img_array=cv2.resize(img_array,(256,256))
                training_data.append([img_array,class_num])
                #print(training_data)
            except Exception as e:
                pass
        count.append(c)
    return count
count_train=create_training_data()

testing_data = []
def create_testing_data():
    count=[]
    for c in categories:
        path=os.path.join(dirtest,c)
        class_num=categories.index(c)
        c=0
        for i in os.listdir(path):
            c=c+1
            try:
                img_array=cv2.imread(os.path.join(path,i))
                img_array=cv2.resize(img_array,(256,256))
                testing_data.append([img_array,class_num])
            except Exception as e:
                pass
        count.append(c)
    return count
count_test=create_testing_data()

print(len(training_data))
print(count_train)
print(len(testing_data))
print(count_test)

random.shuffle(training_data)
random.shuffle(testing_data)

x_train = []
y_train = []
x_test = []
y_test = []

for features, label in training_data:
    x_train.append(features)
    y_train.append(label)
x_train=np.array(x_train).reshape(-1,256,256,3)

for features, label in testing_data:
    x_test.append(features)
    y_test.append(label)
x_test=np.array(x_test).reshape(-1,256,256,3)

x_train.shape

x_test.shape

from keras.utils import to_categorical

y_train_cat=to_categorical(y_train,4)

y_test_cat=to_categorical(y_test,4)

"""AlexNet"""

from keras.models import Sequential

from keras.layers import Activation,Dropout,Flatten,Conv2D,MaxPooling2D,Dense

from keras.layers.normalization import BatchNormalization

# (3) Create a sequential model
model = Sequential()

# 1st Convolutional Layer
model.add(Conv2D(filters=96, input_shape=(256,256,3), kernel_size=(11,11),\
 strides=(4,4), padding='valid'))
model.add(Activation('relu'))
# Pooling 
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
# Batch Normalisation before passing it to the next layer
model.add(BatchNormalization())

# 2nd Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Pooling
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
# Batch Normalisation
model.add(BatchNormalization())

# 3rd Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Batch Normalisation
model.add(BatchNormalization())

# 4th Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Batch Normalisation
model.add(BatchNormalization())

# 5th Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Pooling
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
# Batch Normalisation
model.add(BatchNormalization())

# Passing it to a dense layer
model.add(Flatten())
# 1st Dense Layer
model.add(Dense(4096, input_shape=(128*128*3,)))
model.add(Activation('relu'))
# Add Dropout to prevent overfitting
model.add(Dropout(0.4))
# Batch Normalisation
model.add(BatchNormalization())

# 2nd Dense Layer
model.add(Dense(4096))
model.add(Activation('relu'))
# Add Dropout
model.add(Dropout(0.4))
# Batch Normalisation
model.add(BatchNormalization())

# 3rd Dense Layer
model.add(Dense(1000))
model.add(Activation('relu'))
# Add Dropout
model.add(Dropout(0.4))
# Batch Normalisation
model.add(BatchNormalization())

# Output Layer
model.add(Dense(4))
model.add(Activation('softmax'))

model.summary()

# (4) Compile 
model.compile(loss='categorical_crossentropy', optimizer='adam',\
 metrics=['accuracy'])

# (5) Train
results = model.fit(x_train, y_train_cat, batch_size=64, epochs=250, verbose=1, \
validation_split=0.2, shuffle=True)

loss, acc = model.evaluate(x_test,y_test_cat, verbose=1)
print('Restored model, accuracy: {:5.2f}%'.format(100*acc))

val_acc=results.history['val_accuracy']

print(max(val_acc))

acc=max(results.history['accuracy'])
acc

# summarize history for accuracy
plt.plot(results.history['accuracy'])
plt.plot(results.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

print(results.history.keys())

"""Our Model"""

model = Sequential()

model.add(Conv2D(filters=32,kernel_size=(3,3),padding ='same',input_shape=(256,256,3),activation='relu'))
model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu'))
model.add(MaxPooling2D(pool_size=(8,8)))


model.add(Conv2D(filters=32,kernel_size=(3,3),padding ='same',activation='relu'))
model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu'))
model.add(MaxPooling2D(pool_size=(8,8)))


model.add(Activation('relu'))

model.add(Flatten())

model.add(Dense(256))
model.add(Activation('relu'))

#To avoid overfitting, randomly disables neurons
model.add(Dense(4))  
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',
             optimizer='rmsprop',
             metrics=['accuracy'])

results = model.fit(x_train, y_train_cat, batch_size=64, epochs=250, verbose=1, \
validation_split=0.2, shuffle=True)

loss, acc = model.evaluate(x_test,y_test_cat, verbose=1)
print('Restored model, accuracy: {:5.2f}%'.format(100*acc))

val_acc=results.history['val_accuracy']

acc=results.history['accuracy']

print(max(acc))

max(val_acc)

# summarize history for accuracy
plt.plot(results.history['accuracy'])
plt.plot(results.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

from google.colab import drive
drive.mount('/content/drive')

"""ResNet"""

# Commented out IPython magic to ensure Python compatibility.
import cv2
from PIL import Image
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import os
from urllib.request import urlopen,urlretrieve
from PIL import Image
from tqdm import tqdm_notebook
# %matplotlib inline
from sklearn.utils import shuffle
import cv2


from keras.models import load_model
from sklearn.datasets import load_files   
from keras.utils import np_utils
from glob import glob
from keras import applications
from keras.preprocessing.image import ImageDataGenerator 
from keras import optimizers
from keras.models import Sequential,Model,load_model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D
from keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint
# %matplotlib inline

base_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (256,256,3))

########## ResNet

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.7)(x)
predictions = Dense(4, activation= 'softmax')(x)
model = Model(inputs = base_model.input, outputs = predictions)

from keras.optimizers import SGD, Adam
# sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)
adam = Adam(lr=0.0001)
model.compile(optimizer= adam, loss='categorical_crossentropy', metrics=['accuracy'])

from keras.callbacks import ModelCheckpoint, EarlyStopping

results = model.fit(x_train, y_train_cat, batch_size=32, epochs=50, verbose=1, \
validation_split=0.2, shuffle=True)

loss, acc = model.evaluate(x_test,y_test_cat, verbose=2)
print('Restored model, accuracy: {:5.2f}%'.format(100*acc))

val_acc=results.history['val_accuracy']

acc=results.history['accuracy']

max(val_acc)

max(acc)

!unzip '/content/script.zip'

"""VGG"""

import numpy as np
import os
import time
from vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input
from imagenet_utils import decode_predictions
from keras.layers import Dense, Activation, Flatten
from keras.layers import merge, Input
from keras.models import Model
from keras.utils import np_utils
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

x_train.shape

# Define the number of classes
num_classes = 4

names = ['Anthracnose','Downy Mildew','Healthy','Powdery Mildew']

image_input = Input(shape=(224, 224, 3))

model = VGG16(input_tensor=image_input, include_top=True,weights='imagenet')
model.summary()
last_layer = model.get_layer('fc2').output
#x= Flatten(name='flatten')(last_layer)
out = Dense(num_classes, activation='softmax', name='output')(last_layer)
custom_vgg_model = Model(image_input, out)
custom_vgg_model.summary()

for layer in custom_vgg_model.layers[:-1]:
	layer.trainable = False

custom_vgg_model.layers[3].trainable

custom_vgg_model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])


t=time.time()

hist = custom_vgg_model.fit(x_train, y_train_cat, batch_size=64, epochs=100, verbose=1, validation_data=(x_test, y_test_cat))

print('Training time: %s' % (t - time.time()))
(loss, accuracy) = custom_vgg_model.evaluate(x_test, y_test_cat, batch_size=10, verbose=1)

print(accuracy)

max(hist.history['val_accuracy'])

max(hist.history['accuracy'])

